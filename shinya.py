import streamlit as st
import requests
import sqlite3
import gzip
import shutil
import os
import html
import urllib.parse
import re

DB_GZ_URL = "https://github.com/22552/kasosuta-dataset/releases/download/dai2v1/cmt.db.gz"
DB_FILE = "comments.db"
GZ_FILE = "cmt.db.gz"

# =========================
# DBæº–å‚™
# =========================
def ensure_db():
    if os.path.exists(DB_FILE):
        return

    if not os.path.exists(GZ_FILE):
        r = requests.get(DB_GZ_URL, stream=True, timeout=120)
        r.raise_for_status()
        with open(GZ_FILE, "wb") as f:
            for chunk in r.iter_content(8192):
                f.write(chunk)

    with gzip.open(GZ_FILE, "rb") as f_in:
        with open(DB_FILE, "wb") as f_out:
            shutil.copyfileobj(f_in, f_out)

ensure_db()

# =========================
# SQLiteæ¥ç¶š
# =========================
conn = sqlite3.connect(DB_FILE, check_same_thread=False)
cur = conn.cursor()

# =========================
# UI
# =========================
st.title("Scratch ã‚³ãƒ¡ãƒ³ãƒˆé«˜åº¦æ¤œç´¢")
st.write("å…«æˆ¸å¸‚ã«ã„ã“ã†!")

with st.expander("ğŸ” æ¤œç´¢ã®ä½¿ã„ã‹ãŸ", expanded=False):
    st.markdown("""
    - **ANDæ¤œç´¢**: ã‚¹ãƒšãƒ¼ã‚¹ã§åŒºåˆ‡ã‚‹ã¨ã€Œã™ã¹ã¦å«ã‚€ã€ã«ãªã‚Šã¾ã™ï¼ˆä¾‹: `scratch çŒ«`ï¼‰
    - **é™¤å¤–æ¤œç´¢**: å˜èªã®å‰ã« `-` ã‚’ã¤ã‘ã‚‹ã¨é™¤å¤–ã—ã¾ã™ï¼ˆä¾‹: `scratch -å®£ä¼`ï¼‰
    - **ORæ¤œç´¢**: `|` (ç¸¦æ£’) ã§åŒºåˆ‡ã‚‹ã¨ã€Œã„ãšã‚Œã‹ã‚’å«ã‚€ã€ã«ãªã‚Šã¾ã™ï¼ˆä¾‹: `ãƒã‚°|ä¸å…·åˆ`ï¼‰
    - **è¨˜å·ãƒ»çµµæ–‡å­—**: `>>` ã‚„çµµæ–‡å­—ã‚‚è‡ªå‹•å¤‰æ›ã—ã¦æ¤œç´¢ã—ã¾ã™
    """)

user_q = st.text_input("ãƒ¦ãƒ¼ã‚¶ãƒ¼å")
text_q = st.text_input("æ¤œç´¢ï¼ˆå†…å®¹ï¼‰", placeholder="ä¾‹: ã‚Šã‚“ã” ãƒãƒŠãƒŠ -ã‚¹ã‚¤ã‚«")

if st.button("æ¤œç´¢"):
    query = "SELECT id,user,datetime,content,is_reply,parent_id FROM comments WHERE 1=1"
    params = []

    if user_q:
        query += " AND user LIKE ?"
        params.append(f"%{user_q}%")

    if text_q:
        # ã‚¹ãƒšãƒ¼ã‚¹ï¼ˆå…¨è§’åŠè§’ï¼‰ã§åˆ†å‰²
        words = re.split(r'\s+', text_q.strip())
        
        for word in words:
            if not word: continue
            
            # é™¤å¤–æ¤œç´¢ (å…ˆé ­ãŒãƒã‚¤ãƒŠã‚¹)
            is_exclude = word.startswith('-') and len(word) > 1
            search_word = word[1:] if is_exclude else word
            operator = "NOT LIKE" if is_exclude else "LIKE"
            conjunction = "AND" if is_exclude else "AND" # ANDæ¡ä»¶ã®ä¸­ã§LIKEã‹NOT LIKEã‹

            # ORæ¤œç´¢ (ç¸¦æ£’ | ãŒå«ã¾ã‚Œã‚‹å ´åˆ)
            if '|' in search_word and not is_exclude:
                or_parts = search_word.split('|')
                or_clauses = []
                for p in or_parts:
                    # å„ãƒ‘ãƒ¼ãƒ„ã«å¯¾ã—ã¦é€šå¸¸ãƒ»HTMLãƒ»URLã®3ãƒ‘ã‚¿ãƒ¼ãƒ³ä½œæˆ
                    p_esc = html.escape(p)
                    p_url = urllib.parse.quote(p)
                    or_clauses.append("(content LIKE ? OR content LIKE ? OR content LIKE ?)")
                    params.extend([f"%{p}%", f"%{p_esc}%", f"%{p_url}%"])
                query += f" AND ({' OR '.join(or_clauses)})"
            
            else:
                # é€šå¸¸ã®AND/é™¤å¤–æ¤œç´¢ (é€šå¸¸ãƒ»HTMLãƒ»URLã®3ãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œ)
                w_esc = html.escape(search_word)
                w_url = urllib.parse.quote(search_word)
                
                if is_exclude:
                    # é™¤å¤–ã®å ´åˆã¯ã€Œã©ã‚Œã«ã‚‚å«ã¾ã‚Œãªã„ã€å¿…è¦ãŒã‚ã‚‹
                    query += f" AND (content NOT LIKE ? AND content NOT LIKE ? AND content NOT LIKE ?)"
                else:
                    # å«ã‚€å ´åˆã¯ã€Œã©ã‚Œã‹ã«å«ã¾ã‚Œã‚Œã°è‰¯ã„ã€
                    query += f" AND (content LIKE ? OR content LIKE ? OR content LIKE ?)"
                
                params.extend([f"%{search_word}%", f"%{w_esc}%", f"%{w_url}%"])

    # ä¸¦ã³æ›¿ãˆ
    query += " ORDER BY COALESCE(parent_id, id) DESC, datetime ASC"

    rows = cur.execute(query, params).fetchall()
    st.session_state["rows"] = rows
    st.session_state["page"] = 1

# =========================
# ãƒšãƒ¼ã‚¸è¡¨ç¤º
# =========================
if "rows" in st.session_state:
    rows = st.session_state["rows"]
    page_size = 200
    total_pages = max(1, (len(rows) + page_size - 1) // page_size)

    page = st.number_input("ãƒšãƒ¼ã‚¸", min_value=1, max_value=total_pages, 
                           value=st.session_state.get("page", 1), key="page_input")

    start = (page - 1) * page_size
    end = start + page_size

    st.write(f"çµæœ: {len(rows)} ä»¶ ( {start+1} - {min(end, len(rows))} è¡¨ç¤º )")

    for r in rows[start:end]:
        prefix = "â†³ " if r[4] == 1 else ""
        parent = f"(è¿”ä¿¡å…ˆ: {r[5]})" if r[4] == 1 else ""
        
        # ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦è¡¨ç¤º
        display_content = html.unescape(r[3])
        try:
            display_content = urllib.parse.unquote(display_content)
        except:
            pass
            
        st.write(f"{prefix}ID:{r[0]} [{r[2]}] {r[1]}: {display_content} {parent}")
